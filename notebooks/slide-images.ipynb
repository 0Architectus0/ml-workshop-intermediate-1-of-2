{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"notebook\", font_scale=1.4,\n",
    "              rc={\"figure.figsize\": [10, 6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = np.linspace(0, 8, 100)\n",
    "train = 1./(1 + np.exp(-line))\n",
    "plt.plot(train, label='training accuracy')\n",
    "gen_true = - (line/ 10) ** 2 + (line/10 - .5) ** 3 + 1\n",
    "gen_true = train - (line/15) ** 2 - .2\n",
    "plt.plot(gen_true, label=\"generalization\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model Complexity\")\n",
    "plt.legend()\n",
    "plt.savefig(\"images/overfitting_validation_set_1.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "plt.plot(train, label='training accuracy')\n",
    "plt.plot(gen_true, label=\"generalization\")\n",
    "validation_set = gen_true + rng.normal(scale=.07, size=100)\n",
    "plt.plot(validation_set, label=\"validation set\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model Complexity\")\n",
    "plt.legend()\n",
    "plt.savefig(\"images/overfitting_validation_set_2.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approval = pd.read_csv(\"data/approval_topline.csv\", parse_dates=['timestamp'], index_col='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approval_est = approval.loc[approval['subgroup'] == \"Adults\", [\"approve_estimate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approval_est.to_csv(\"data/approval_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "approval_est.plot(ax=ax)\n",
    "ax.set_title(\"Approval Ratings\")\n",
    "fig.savefig(\"images/approval_ratings.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "rng = np.random.RandomState(42)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "approval_est.plot(ax=ax)\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "for i in range(20):\n",
    "    rect = Rectangle((rng.randint(xlim[0], xlim[1]), ylim[0]), 10,\n",
    "                     ylim[1]-ylim[0], facecolor='#FFAAAA', alpha=0.8)\n",
    "    ax.add_artist(rect)\n",
    "    \n",
    "# plt.title(\"Presidential approval estimates by fivethirtyeight\")\n",
    "plt.legend([rect], ['Random Test Set'] )\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Approval Ratings\")\n",
    "fig.savefig(\"images/approval_ratings_random.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "approval_est.plot()\n",
    "\n",
    "ax = plt.gca()\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "rect = Rectangle((xlim[1] - 300, ylim[0]), 300, ylim[1]-ylim[0], facecolor='#FFAAAA', alpha=0.8)\n",
    "ax.add_artist(rect)\n",
    "\n",
    "plt.title(\"Approvael Ratings\")\n",
    "plt.legend([rect], ['Structured Test Set'] )\n",
    "ax.set_xlabel(\"\")\n",
    "plt.savefig(\"images/approval_ratings_structured.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop-intermediate-1-of-2",
   "language": "python",
   "name": "conda-env-ml-workshop-intermediate-1-of-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
